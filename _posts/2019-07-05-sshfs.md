---
layout: post
title:  "Mount a remote filesystem on a local machine with SSHFS"
date:   2019-07-05
categories: [note]
tags:   [utility]
---
## Overview

Development work at Amazon is usually done on a cloud desktop which SDEs use SSH to communicate. Sometimes I want to view some code coverage reports in HTML format after running unit tests. This can be done is two ways:

1. Copy the report file onto a location on the local machine and use web browser to view it.

2. Log into the remote machine with Remote Desktop software (e.g. NoMachine) to view the report there.

Today I found out that we can use SSHFS to mount a remote filesystem on a local machine so that we can view and edit files as if we are viewing/editing local files.

## Description

SSH is a secure protocol for communicating between machines. SSHFS is a tool that uses SSH to enable mounting of a remote filesystem on a local machine; the network is (mostly) transparent to the user. Because SSHFS authenticates connections, you can be sure that only those who should have access to remote directories can mount them (as long as everything is configured properly).

Because SSH encrypts connections, no one can see your files as they are transferred over the network. And because SSHFS is built using FUSE, even your own root user can only see your files by logging in to your account with su.

## Installation and Setup

### Ubuntu

Execute the following command to install SSHFS:

```bash
sudo apt install sshfs
```

## Command-line Usage

Now, assuming that you have an SSH server running on a remote machine, simply run the SSHFS command to mount the remote directory. In this example, the remote directory is `/workplace/lingnanl/` and the local directory is `~/RemoteWorkspaces/`.

```bash
# Create the local mount point
mkdir ~/RemoteWorkspaces/
# Mount the remote directory
sshfs lingnanl@lingnanl.aka.corp.amazon.com:/workplace/lingnanl/ ~/RemoteWorkspaces/
```

One thing to be aware of is that your UID (User ID, the unique number of your user on a system) is not necessarily the same on the two hosts. When you `ls -l`, the user name associated with each file is printed in the third column. However, in the filesystem, only UIDs are stored, and `ls` simply looks up the UID and finds the user name associated with it. In Unix, UIDs are what matter, not the user names. So if you're `1000` on the local host and `1003` on the remote host, the sshfs mounted directory would show a different user name for your files. This is not a problem, though, because the ssh server on the remote machine is what is actually reading and writing files. So even though it shows up in `ls -l` as a different UID, any changes will be done through the ssh server on the remote host, which will use the correct UID for the remote machine. Problems may arise if you attempt to use a program that looks at UIDs of files (e.g. `ls` prints the wrong user name).

The `idmap=user` option ensures that files owned by the remote user are owned by the local user. If you don't use `idmap=user`, files in the mounted directory might appear to be owned by someone else, because your computer and the remote computer have different ideas about the numeric user ID associated with each user name. `idmap=user` will not translate UIDs for other users.

---

## References

* [SSHFS](https://help.ubuntu.com/community/SSHFS)
